{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31f520a6-6a44-41b3-9b2a-84b2eb191472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.3\n",
      "2.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "print(pyspark.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "273c456e-9ec8-4131-826d-a658a429a4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/03/26 23:58:36 WARN Utils: Your hostname, Trydex resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/03/26 23:58:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/26 23:58:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7efe46026f10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "# `.master(\"local[*]\")` => Use local resources 'local' with all available cores '[*]'\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe1b232-cc5e-47f9-add7-60df3925997d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRun this block if you need to download the datasets and unzip them, Ignore if already exists\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run this block if you need to download the datasets and unzip them, Ignore if already exists\n",
    "\"\"\"\n",
    "# !wget -P ../datasets https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz\n",
    "# !gunzip ../datasets/fhvhv_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b283f205-2523-40ed-92e3-b28b8be4ac3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('SR_Flag', StringType(), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv file into spark\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('../datasets/fhvhv_tripdata_2021-01.csv')\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749f496b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nExtract 100 lines of data from csv to avoid using the entire data and save it as 'head.csv'\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract 100 lines of data from csv to avoid using the entire data and save it as 'head.csv'\n",
    "\"\"\"\n",
    "# !head -n 101 '../datasets/fhvhv_tripdata_2021-01.csv' > '../datasets/head.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "061c290a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv as pandas Dataframe\n",
    "df_pandas = pd.read_csv('../datasets/head.csv')\n",
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e300bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pandas Dataframe into Spark Dataframe\n",
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bb2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "# Set Schema of the attributes\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c0114ae-9b06-49e4-8a00-588eef0de356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('SR_Flag', StringType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv and apply schema\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('../datasets/fhvhv_tripdata_2021-01.csv')\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9de06da0-3130-4416-a6ff-4aaa7317a027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Check number of partitions created for the csv file\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0090061-e57e-473f-bdab-8f6e8018cf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int, SR_Flag: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the number of partitions for the spark Dataframe\n",
    "df = df.repartition(30)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24d2273e-6e77-424a-b0bc-5ebbaa6318c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==>                                                      (1 + 19) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================>                                  (8 + 12) / 20]\r"
     ]
    }
   ],
   "source": [
    "# New updated partitions for the spark dataframe\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f0c187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save file as parquet\n",
    "# df.write.parquet('../artifacts/fhvhv_2021_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15383acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2021-01-01 19:42:07|2021-01-01 19:44:59|          74|          74|   null|\n",
      "|           HV0003|              B02875|2021-01-01 08:21:01|2021-01-01 08:32:29|          18|         185|   null|\n",
      "|           HV0003|              B02887|2021-01-01 21:38:30|2021-01-01 21:56:11|          69|         185|   null|\n",
      "|           HV0005|              B02510|2021-01-02 14:24:10|2021-01-02 14:30:53|         225|         177|   null|\n",
      "|           HV0005|              B02510|2021-01-01 07:50:30|2021-01-01 08:19:52|         191|         244|   null|\n",
      "|           HV0003|              B02872|2021-01-01 21:07:14|2021-01-01 21:15:05|          95|         130|   null|\n",
      "|           HV0003|              B02882|2021-01-01 17:52:35|2021-01-01 18:05:11|         141|          48|   null|\n",
      "|           HV0005|              B02510|2021-01-01 18:51:49|2021-01-01 19:16:43|         129|         265|   null|\n",
      "|           HV0003|              B02866|2021-01-02 07:56:58|2021-01-02 08:11:46|         215|          93|   null|\n",
      "|           HV0005|              B02510|2021-01-01 12:34:37|2021-01-01 12:45:51|          97|         256|   null|\n",
      "|           HV0003|              B02887|2021-01-01 16:52:10|2021-01-01 16:59:45|         107|         229|   null|\n",
      "|           HV0003|              B02512|2021-01-01 22:09:21|2021-01-01 22:38:04|          48|         223|   null|\n",
      "|           HV0003|              B02764|2021-01-01 01:51:41|2021-01-01 01:58:13|         225|         225|   null|\n",
      "|           HV0003|              B02764|2021-01-02 11:30:20|2021-01-02 11:34:44|          47|          47|   null|\n",
      "|           HV0003|              B02882|2021-01-01 14:55:59|2021-01-01 15:26:36|          17|         263|   null|\n",
      "|           HV0003|              B02764|2021-01-01 17:52:59|2021-01-01 18:12:48|         234|         262|   null|\n",
      "|           HV0003|              B02765|2021-01-01 16:45:47|2021-01-01 17:09:39|          41|          68|   null|\n",
      "|           HV0003|              B02617|2021-01-01 00:23:11|2021-01-01 00:45:46|         100|         243|   null|\n",
      "|           HV0003|              B02875|2021-01-02 16:08:15|2021-01-02 16:13:52|          34|          49|   null|\n",
      "|           HV0003|              B02884|2021-01-02 12:27:13|2021-01-02 12:39:29|         186|         125|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the saved parquet file\n",
    "df = spark.read.parquet('../artifacts/fhvhv_2021_01/')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b44743b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea5c0e",
   "metadata": {},
   "source": [
    "### **Actions vs Transformations**\n",
    "\n",
    "**Actions - (Eager)**\n",
    "- These are executed by spark immediately when called\n",
    "- .show(), take(), head(), tail(), write(), read(), etc.\n",
    "\n",
    "**Transformations - (Lazy)**\n",
    "- These are waiting to be executed by spark and is merely an instruction\n",
    "- .filter(), .select(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7902d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-01 19:42:07|2021-01-01 19:44:59|          74|          74|\n",
      "|2021-01-01 08:21:01|2021-01-01 08:32:29|          18|         185|\n",
      "|2021-01-01 21:38:30|2021-01-01 21:56:11|          69|         185|\n",
      "|2021-01-02 14:24:10|2021-01-02 14:30:53|         225|         177|\n",
      "|2021-01-01 07:50:30|2021-01-01 08:19:52|         191|         244|\n",
      "|2021-01-01 21:07:14|2021-01-01 21:15:05|          95|         130|\n",
      "|2021-01-01 17:52:35|2021-01-01 18:05:11|         141|          48|\n",
      "|2021-01-01 18:51:49|2021-01-01 19:16:43|         129|         265|\n",
      "|2021-01-02 07:56:58|2021-01-02 08:11:46|         215|          93|\n",
      "|2021-01-01 12:34:37|2021-01-01 12:45:51|          97|         256|\n",
      "|2021-01-01 16:52:10|2021-01-01 16:59:45|         107|         229|\n",
      "|2021-01-01 22:09:21|2021-01-01 22:38:04|          48|         223|\n",
      "|2021-01-01 01:51:41|2021-01-01 01:58:13|         225|         225|\n",
      "|2021-01-02 11:30:20|2021-01-02 11:34:44|          47|          47|\n",
      "|2021-01-01 14:55:59|2021-01-01 15:26:36|          17|         263|\n",
      "|2021-01-01 17:52:59|2021-01-01 18:12:48|         234|         262|\n",
      "|2021-01-01 16:45:47|2021-01-01 17:09:39|          41|          68|\n",
      "|2021-01-01 00:23:11|2021-01-01 00:45:46|         100|         243|\n",
      "|2021-01-02 16:08:15|2021-01-02 16:13:52|          34|          49|\n",
      "|2021-01-02 12:27:13|2021-01-02 12:39:29|         186|         125|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform select\n",
    "df_sel = df.select('pickup_datetime','dropoff_datetime','PULocationID', 'DOLocationID')\n",
    "df_sel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2c65af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-01 19:42:07|2021-01-01 19:44:59|          74|          74|\n",
      "|2021-01-01 18:52:48|2021-01-01 18:57:22|          74|          75|\n",
      "|2021-01-01 15:10:03|2021-01-01 15:12:57|          74|          75|\n",
      "|2021-01-02 06:24:16|2021-01-02 06:35:24|          74|          59|\n",
      "|2021-01-01 19:17:03|2021-01-01 19:39:43|          74|         185|\n",
      "|2021-01-01 03:31:49|2021-01-01 03:43:12|          74|         169|\n",
      "|2021-01-01 07:36:11|2021-01-01 07:48:29|          74|         141|\n",
      "|2021-01-01 18:44:44|2021-01-01 18:59:15|          74|         167|\n",
      "|2021-01-01 01:09:09|2021-01-01 01:14:02|          74|          75|\n",
      "|2021-01-02 12:31:31|2021-01-02 12:45:13|          74|         151|\n",
      "|2021-01-01 16:08:23|2021-01-01 16:12:06|          74|          74|\n",
      "|2021-01-01 14:32:29|2021-01-01 14:54:20|          74|         164|\n",
      "|2021-01-01 04:15:02|2021-01-01 04:32:26|          74|         213|\n",
      "|2021-01-01 23:28:19|2021-01-01 23:48:53|          74|         241|\n",
      "|2021-01-02 08:59:51|2021-01-02 09:10:18|          74|         244|\n",
      "|2021-01-02 15:05:35|2021-01-02 15:30:25|          74|          48|\n",
      "|2021-01-01 20:47:07|2021-01-01 21:09:16|          74|          48|\n",
      "|2021-01-01 00:20:26|2021-01-01 00:29:25|          74|          41|\n",
      "|2021-01-02 11:57:26|2021-01-02 12:05:10|          74|          42|\n",
      "|2021-01-01 00:45:21|2021-01-01 01:15:33|          74|          60|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform filter\n",
    "df_filter = df_sel.filter(df.PULocationID == 74)\n",
    "df_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef4f5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------+------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+-----------+------------+------------+------------+\n",
      "|2021-01-01 19:42:07|2021-01-01 19:44:59| 2021-01-01|  2021-01-01|          74|          74|\n",
      "|2021-01-01 08:21:01|2021-01-01 08:32:29| 2021-01-01|  2021-01-01|          18|         185|\n",
      "|2021-01-01 21:38:30|2021-01-01 21:56:11| 2021-01-01|  2021-01-01|          69|         185|\n",
      "|2021-01-02 14:24:10|2021-01-02 14:30:53| 2021-01-02|  2021-01-02|         225|         177|\n",
      "|2021-01-01 07:50:30|2021-01-01 08:19:52| 2021-01-01|  2021-01-01|         191|         244|\n",
      "|2021-01-01 21:07:14|2021-01-01 21:15:05| 2021-01-01|  2021-01-01|          95|         130|\n",
      "|2021-01-01 17:52:35|2021-01-01 18:05:11| 2021-01-01|  2021-01-01|         141|          48|\n",
      "|2021-01-01 18:51:49|2021-01-01 19:16:43| 2021-01-01|  2021-01-01|         129|         265|\n",
      "|2021-01-02 07:56:58|2021-01-02 08:11:46| 2021-01-02|  2021-01-02|         215|          93|\n",
      "|2021-01-01 12:34:37|2021-01-01 12:45:51| 2021-01-01|  2021-01-01|          97|         256|\n",
      "|2021-01-01 16:52:10|2021-01-01 16:59:45| 2021-01-01|  2021-01-01|         107|         229|\n",
      "|2021-01-01 22:09:21|2021-01-01 22:38:04| 2021-01-01|  2021-01-01|          48|         223|\n",
      "|2021-01-01 01:51:41|2021-01-01 01:58:13| 2021-01-01|  2021-01-01|         225|         225|\n",
      "|2021-01-02 11:30:20|2021-01-02 11:34:44| 2021-01-02|  2021-01-02|          47|          47|\n",
      "|2021-01-01 14:55:59|2021-01-01 15:26:36| 2021-01-01|  2021-01-01|          17|         263|\n",
      "|2021-01-01 17:52:59|2021-01-01 18:12:48| 2021-01-01|  2021-01-01|         234|         262|\n",
      "|2021-01-01 16:45:47|2021-01-01 17:09:39| 2021-01-01|  2021-01-01|          41|          68|\n",
      "|2021-01-01 00:23:11|2021-01-01 00:45:46| 2021-01-01|  2021-01-01|         100|         243|\n",
      "|2021-01-02 16:08:15|2021-01-02 16:13:52| 2021-01-02|  2021-01-02|          34|          49|\n",
      "|2021-01-02 12:27:13|2021-01-02 12:39:29| 2021-01-02|  2021-01-02|         186|         125|\n",
      "+-------------------+-------------------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Apply a to_date transformation to columns\n",
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .select(\n",
    "        'pickup_datetime',\n",
    "        'dropoff_datetime',\n",
    "        'pickup_date',\n",
    "        'dropoff_date',\n",
    "        'PULocationID', \n",
    "        'DOLocationID',\n",
    "        ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb0eee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s/b44'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User defined functions\n",
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'\n",
    "crazy_stuff('B02884')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2705457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register python function as Spark's user defined function\n",
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf58e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num|base_id|\n",
      "+--------------------+-------+\n",
      "|              B02510|  e/9ce|\n",
      "|              B02875|  e/b3b|\n",
      "|              B02887|  e/b47|\n",
      "|              B02510|  e/9ce|\n",
      "|              B02510|  e/9ce|\n",
      "|              B02872|  e/b38|\n",
      "|              B02882|  e/b42|\n",
      "|              B02510|  e/9ce|\n",
      "|              B02866|  e/b32|\n",
      "|              B02510|  e/9ce|\n",
      "|              B02887|  e/b47|\n",
      "|              B02512|  e/9d0|\n",
      "|              B02764|  e/acc|\n",
      "|              B02764|  e/acc|\n",
      "|              B02882|  e/b42|\n",
      "|              B02764|  e/acc|\n",
      "|              B02765|  s/acd|\n",
      "|              B02617|  e/a39|\n",
      "|              B02875|  e/b3b|\n",
      "|              B02884|  s/b44|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply a udf transformation to column\n",
    "df \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select(\n",
    "        'dispatching_base_num',\n",
    "        'base_id'\n",
    "        ) \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
