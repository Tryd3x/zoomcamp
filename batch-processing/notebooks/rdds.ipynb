{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe85d5c6c40>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, types\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet(\"../datasets/pq/green/*/*\")\n",
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 11, 4, 5, 54), PULocationID=129, total_amount=8.51),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 7, 19, 34, 40), PULocationID=82, total_amount=8.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 19, 7, 7), PULocationID=75, total_amount=8.76),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 27, 17, 26, 4), PULocationID=210, total_amount=11.44),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 19, 12, 18, 31), PULocationID=129, total_amount=5.8),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 22, 19, 43, 11), PULocationID=65, total_amount=26.55),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 7, 13, 39), PULocationID=82, total_amount=18.94),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 14, 17, 55, 4), PULocationID=260, total_amount=11.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 24, 8, 16), PULocationID=59, total_amount=23.58),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 18, 45, 2), PULocationID=97, total_amount=15.3)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df_green \\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "    .rdd\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start = datetime(year=2021,month=1,day=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2021, 10, 29, 13, 51, 40), PULocationID=166, total_amount=15.3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying filter\n",
    "rdd.filter(filter_outliers).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 11, 4, 5, 54), PULocationID=129, total_amount=8.51)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract a row\n",
    "row = rdd.take(1)[0]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((datetime.datetime(2020, 1, 11, 4, 0), 129), (8.51, 1))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the equivalent of groupBy but performing on RDD level\n",
    "# Performing map() to create key value pairs\n",
    "\n",
    "def prepare_for_grouping(row): \n",
    "    # Create a key\n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour, zone) # Composite Key\n",
    "    \n",
    "    # Create value\n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count) # Composite Value\n",
    "\n",
    "    return (key, value) # Returns a tuple\n",
    "\n",
    "prepare_for_grouping(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{key, value}\n",
      "((hour, zone), (amount, count))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2021, 10, 29, 13, 0), 166), (15.3, 1)),\n",
       " ((datetime.datetime(2021, 10, 19, 10, 0), 188), (18.5, 1)),\n",
       " ((datetime.datetime(2021, 10, 14, 8, 0), 75), (9.3, 1)),\n",
       " ((datetime.datetime(2021, 10, 14, 2, 0), 264), (18.36, 1)),\n",
       " ((datetime.datetime(2021, 10, 8, 23, 0), 82), (12.25, 1)),\n",
       " ((datetime.datetime(2021, 10, 26, 16, 0), 95), (12.3, 1)),\n",
       " ((datetime.datetime(2021, 10, 18, 9, 0), 7), (21.96, 1)),\n",
       " ((datetime.datetime(2021, 10, 18, 12, 0), 75), (7.8, 1)),\n",
       " ((datetime.datetime(2021, 10, 17, 10, 0), 35), (23.98, 1)),\n",
       " ((datetime.datetime(2021, 10, 9, 16, 0), 19), (48.11, 1))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map Phase\n",
    "\"\"\"\n",
    "Creates key value pairs for aggregation via map()\n",
    "\"\"\"\n",
    "print(\"{key, value}\")\n",
    "print(\"((hour, zone), (amount, count))\")\n",
    "rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the equivalent of groupBy but performing on RDD level\n",
    "# Performing reduce() to collect values of like keys together (via reduceByKey for local aggregation)\n",
    "\n",
    "# Take 2 tuples with like keys\n",
    "# (K1, V1), (K1, V2)\n",
    "# left_value = V1, right_value = V2\n",
    "def calculate_revenue(left_value, right_value):\n",
    "    # left  => V1 = (amount, count)\n",
    "    # right => V2 = (amount, count)\n",
    "    left_amount, left_count = left_value # amount\n",
    "    right_amount, right_count = right_value # count\n",
    "    \n",
    "    output_amount = left_amount + right_amount # add amount of the same key\n",
    "    output_count = left_count + right_count # add count of the same key\n",
    "    \n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{key, value}\n",
      "((hour, zone), (amount, count))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2021, 10, 16, 7, 0), 174), (162.04000000000002, 3)),\n",
       " ((datetime.datetime(2021, 10, 29, 14, 0), 7), (312.9, 14)),\n",
       " ((datetime.datetime(2021, 10, 5, 8, 0), 152), (120.87, 5)),\n",
       " ((datetime.datetime(2021, 10, 14, 15, 0), 166), (271.59000000000003, 12)),\n",
       " ((datetime.datetime(2021, 10, 13, 14, 0), 42), (109.07, 6)),\n",
       " ((datetime.datetime(2021, 10, 20, 21, 0), 95), (37.739999999999995, 3)),\n",
       " ((datetime.datetime(2021, 10, 28, 10, 0), 75), (515.07, 28)),\n",
       " ((datetime.datetime(2021, 10, 29, 20, 0), 74), (244.71999999999997, 15)),\n",
       " ((datetime.datetime(2021, 10, 10, 7, 0), 69), (58.04, 1)),\n",
       " ((datetime.datetime(2021, 10, 3, 18, 0), 42), (131.51999999999998, 5))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce Phase\n",
    "\"\"\"\n",
    "Perform local aggreagation (sum) on keys via reduceByKey()\n",
    "\"\"\"\n",
    "print(\"{key, value}\")\n",
    "print(\"((hour, zone), (amount, count))\")\n",
    "\n",
    "rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an immutable named tuple instead of using a fully fledged class\n",
    "from collections import namedtuple\n",
    "\n",
    "RevenueRow = namedtuple('RevenueRow',['hour','zone','revenue','count'])\n",
    "\n",
    "# Utility function\n",
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], \n",
    "        zone=row[0][1],\n",
    "        revenue=row[1][0],\n",
    "        count=row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:=========================================>              (14 + 5) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+-----+\n",
      "|               hour|zone|           revenue|count|\n",
      "+-------------------+----+------------------+-----+\n",
      "|2021-10-16 07:00:00| 174|162.04000000000002|    3|\n",
      "|2021-10-29 14:00:00|   7|             312.9|   14|\n",
      "|2021-10-05 08:00:00| 152|            120.87|    5|\n",
      "|2021-10-14 15:00:00| 166|271.59000000000003|   12|\n",
      "|2021-10-13 14:00:00|  42|            109.07|    6|\n",
      "|2021-10-20 21:00:00|  95|37.739999999999995|    3|\n",
      "|2021-10-28 10:00:00|  75|            515.07|   28|\n",
      "|2021-10-29 20:00:00|  74|244.71999999999997|   15|\n",
      "|2021-10-10 07:00:00|  69|             58.04|    1|\n",
      "|2021-10-03 18:00:00|  42|131.51999999999998|    5|\n",
      "+-------------------+----+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Unwrapping each row and converting rdd back to spark DF\n",
    "results_df = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF()\n",
    "results_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hour: timestamp (nullable = true)\n",
      " |-- zone: long (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Automatically inferred schema\n",
    "results_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply a schema\n",
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True),\n",
    "    types.StructField('zone', types.IntegerType(), True),\n",
    "    types.StructField('revenue', types.DoubleType(), True),\n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hour: timestamp (nullable = true)\n",
      " |-- zone: integer (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply schema\n",
    "results_df = (\n",
    "    rdd\n",
    "    .filter(filter_outliers)           # Remove rows with revenue/count outliers\n",
    "    .map(prepare_for_grouping)         # Convert row into (key, value) for aggregation\n",
    "    .reduceByKey(calculate_revenue)    # Combine values by key to calculate revenue\n",
    "    .map(unwrap)                       # Convert (key, value) back to Row\n",
    "    .toDF(result_schema)               # Convert RDD[Row] to DataFrame\n",
    ")\n",
    "\n",
    "results_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 44.71% for 17 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 42.22% for 18 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 40.00% for 19 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 42.22% for 18 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 44.71% for 17 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/04/05 04:14:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save as parquet file\n",
    "results_df.write.parquet('../datasets/tmp/green-revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
