{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your Google Cloud service account credentials JSON file\n",
    "credentials_location = '/home/hyderreza/.google/credentials/google_credentials.json'\n",
    "\n",
    "# Create SparkConf object to configure Spark settings\n",
    "conf = (\n",
    "    SparkConf() \n",
    "    .setMaster('local[*]')  # Run Spark locally using all available CPU cores\n",
    "    .setAppName('test')  # Name of the Spark application\n",
    "    .set(\"spark.jars\", \"../lib/gcs-connector-hadoop3-2.2.5.jar\")  # Add the GCS connector JAR\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")  # Enable service account authentication\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)  # Path to credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/05 23:41:04 WARN Utils: Your hostname, Trydex resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/05 23:41:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/04/05 23:41:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkContext with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Get Hadoop configuration object to set file system-specific settings\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "# Define the abstract file system implementation for GCS\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Define the actual GCS file system implementation class\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "\n",
    "# Set the path to your credentials again for the Hadoop config (redundant but sometimes required)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "\n",
    "# Enable service account authentication for the Hadoop-level GCS configuration\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f62b815fb80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a SparkSession using the same configuration as SparkContext\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config(conf=sc.getConf())  # Reuse the SparkConf from the SparkContext\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2020-01-11 04:05:54|  2020-01-11 04:13:49|                 N|       1.0|         129|         129|            1.0|         0.81|        6.5|  0.5|    0.5|      0.71|         0.0|     null|                  0.3|        8.51|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-07 19:34:40|  2020-01-07 19:43:01|                 N|       1.0|          82|          56|            1.0|         1.25|        7.0|  1.0|    0.5|       0.0|         0.0|     null|                  0.3|         8.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-15 19:07:07|  2020-01-15 19:12:56|                 N|       1.0|          75|          75|            1.0|         0.88|        5.5|  1.0|    0.5|      1.46|         0.0|     null|                  0.3|        8.76|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-27 17:26:04|  2020-01-27 17:31:29|                 N|       1.0|         210|         108|            1.0|         1.52|        7.0|  1.0|    0.5|      2.64|         0.0|     null|                  0.3|       11.44|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-19 12:18:31|  2020-01-19 12:23:29|                 N|       1.0|         129|         129|            1.0|         0.58|        5.0|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|         5.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-22 19:43:11|  2020-01-22 20:11:30|                 N|       1.0|          65|          48|            1.0|         5.27|       22.0|  1.0|    0.5|       0.0|         0.0|     null|                  0.3|       26.55|         2.0|      1.0|                2.75|\n",
      "|       2| 2020-01-07 13:39:00|  2020-01-07 13:52:00|              null|      null|          82|          56|           null|         1.96|      15.89| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       18.94|        null|     null|                null|\n",
      "|       2| 2020-01-14 17:55:04|  2020-01-14 18:06:56|                 N|       1.0|         260|         129|            6.0|         1.77|        9.5|  1.0|    0.5|       0.0|         0.0|     null|                  0.3|        11.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-24 08:16:00|  2020-01-24 08:33:00|              null|      null|          59|         126|           null|         2.33|      20.33| 2.75|    0.5|       0.0|         0.0|     null|                  0.0|       23.58|        null|     null|                null|\n",
      "|       1| 2020-01-15 18:45:02|  2020-01-15 19:03:00|                 N|       1.0|          97|          89|            2.0|          3.0|       13.5|  1.0|    0.5|       0.0|         0.0|     null|                  0.3|        15.3|         2.0|      1.0|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now you can access buckets in google cloud storage via gs://\n",
    "df_green = spark.read.parquet('gs://zoomcamp-454219-nytaxi/pq/green/*/*')\n",
    "df_green.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
